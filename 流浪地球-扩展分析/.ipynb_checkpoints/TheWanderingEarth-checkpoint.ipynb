{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分析可视化\n",
    "import os#系统模块\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import re#热力图\n",
    "from pyecharts import Line, Geo, Bar, Pie, Page, ThemeRiver\n",
    "from snownlp import SnowNLP#NLP模块，用于语言处理，情感分析\n",
    "import jieba#Jieba模块\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator#词云模块\n",
    "\n",
    "fth = open('D:\\Program Code\\Python\\Jupyter\\课程设计\\流浪地球\\pyecharts_citys_supported.txt', 'r', encoding='utf-8').read() # pyecharts支持城市列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤字符串只保留中文\n",
    "def translate(str):\n",
    "    line = str.strip()\n",
    "    p2 = re.compile('[^\\u4e00-\\u9fa5]')   # 中文的编码范围是：\\u4e00到\\u9fa5\n",
    "    zh = \" \".join(p2.split(line)).strip()\n",
    "    zh = \",\".join(zh.split())\n",
    "    str = re.sub(\"[A-Za-z0-9!！，%\\[\\],。]\", \"\", zh)\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是按照列属性读取的\n",
    "def count_sentiment(csv_file):\n",
    "    path = os.path.abspath(os.curdir)\n",
    "    csv_file = path+ \"\\\\\" + csv_file + \".csv\"\n",
    "    csv_file = csv_file.replace('\\\\', '\\\\\\\\')\n",
    "    d = pd.read_csv(csv_file, engine='python', encoding='utf-8')\n",
    "    motion_list = []\n",
    "    for i in d['content']:\n",
    "        try:\n",
    "            s = round(SnowNLP(i).sentiments, 2)\n",
    "            motion_list.append(s)\n",
    "        except TypeError:\n",
    "            continue\n",
    "    result = {}\n",
    "    for i in set(motion_list):\n",
    "        result[i] = motion_list.count(i)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#情感分析曲线\n",
    "def draw_sentiment_pic(csv_file):\n",
    "    attr, val = [], []\n",
    "    info = count_sentiment(csv_file)\n",
    "    info = sorted(info.items(), key=lambda x: x[0], reverse=False)  # dict的排序方法\n",
    "    for each in info[:-1]:\n",
    "        attr.append(each[0])\n",
    "        val.append(each[1])\n",
    "    line = Line(csv_file+\":影评情感分析\")\n",
    "    line.add(\"\", attr, val, is_smooth=True, is_more_utils=True)\n",
    "    line.render(csv_file+\"_情感分析曲线图.html\")\n",
    "    line.render_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成词云图片\n",
    "def word_cloud(csv_file, stopwords_path, pic_path):\n",
    "    pic_name = csv_file+\"_词云图.png\"\n",
    "    path = os.path.abspath(os.curdir)\n",
    "    csv_file = path+ \"\\\\\" + csv_file + \".csv\"\n",
    "    csv_file = csv_file.replace('\\\\', '\\\\\\\\')\n",
    "    d = pd.read_csv(csv_file, engine='python', encoding='utf-8')\n",
    "    content = []\n",
    "    for i in d['content']:\n",
    "        try:\n",
    "            i = translate(i)\n",
    "        except AttributeError as e:\n",
    "            continue\n",
    "        else:\n",
    "            content.append(i)\n",
    "    comment_after_split = jieba.cut(str(content), cut_all=False)\n",
    "    wl_space_split = \" \".join(comment_after_split)\n",
    "    backgroud_Image = plt.imread(pic_path)\n",
    "    stopwords = STOPWORDS.copy()\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        for i in f.readlines():\n",
    "            stopwords.add(i.strip('\\n'))\n",
    "        f.close()\n",
    "\n",
    "    wc = WordCloud(width=1024, height=768, background_color='white',\n",
    "                   mask=backgroud_Image, font_path=\"C:\\Windows\\Fonts\\msyh.ttc\",\n",
    "                   stopwords=stopwords, max_font_size=400,\n",
    "                   random_state=50)\n",
    "    wc.generate_from_text(wl_space_split)\n",
    "    img_colors = ImageColorGenerator(backgroud_Image)\n",
    "    wc.recolor(color_func=img_colors)\n",
    "    plt.imshow(wc)\n",
    "    plt.axis('off')  \n",
    "    plt.show() \n",
    "    wc.to_file(pic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#城市统计，统计评论的来源\n",
    "def count_city(csv_file):\n",
    "    path = os.path.abspath(os.curdir)\n",
    "    csv_file = path+ \"\\\\\" + csv_file +\".csv\"\n",
    "    csv_file = csv_file.replace('\\\\', '\\\\\\\\')\n",
    "    \n",
    "    d = pd.read_csv(csv_file, engine='python', encoding='utf-8')\n",
    "    city = [translate(n) for n in d['city'].dropna()] # 清洗城市，将中文城市提取出来并删除标点符号等 \n",
    "    \n",
    "    # 这是从网上找的省份的名称，将其转换成列表的形式\n",
    "    province = '湖南,湖北,广东,广西、河南、河北、山东、山西,江苏、浙江、江西、黑龙江、新疆,云南、贵州、福建、吉林、安徽,四川、西藏、宁夏、辽宁、青海、甘肃、陕西,内蒙古、台湾,海南'\n",
    "    province = province.replace('、',',').split(',')\n",
    "    rep_province = \"|\".join(province) # re.sub中城市替换的条件\n",
    "    \n",
    "    All_city = jieba.cut(\"\".join(city)) # 分词，将省份和市级地名分开，当然有一些如吉林长春之类没有很好的分开，因此我们需要用re.sub（）来将之中的省份去除掉\n",
    "    final_city= []\n",
    "    for a_city in All_city:\n",
    "        a_city_sub = re.sub(rep_province,\"\",a_city) # 对每一个单元使用sub方法，如果有省份的名称，就将他替换为“”（空）\n",
    "        if a_city_sub == \"\": # 判断，如果为空，跳过\n",
    "            continue\n",
    "        elif a_city_sub in fth: # 因为所有的省份都被排除掉了，便可以直接判断城市在不在列表之中，如果在，final_city便增加\n",
    "            final_city.append(a_city_sub)\n",
    "        else: # 不在fth中的城市，跳过\n",
    "            continue\n",
    "            \n",
    "    result = {}\n",
    "    print(\"城市总数量为：\",len(final_city))\n",
    "    for i in set(final_city):\n",
    "        result[i] = final_city.count(i)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_citys_pic(csv_file):\n",
    "    page = Page(csv_file+\":评论城市分析\")\n",
    "    info = count_city(csv_file)\n",
    "    geo = Geo(\"\",\"非本人原创\",title_pos=\"center\", width=1200,height=600, background_color='#404a59', title_color=\"#fff\")\n",
    "    while True:   # 二次筛选，和pyecharts支持的城市库进行匹配，如果报错则删除该城市对应的统计\n",
    "        try:\n",
    "            attr, val = geo.cast(info)\n",
    "            geo.add(\"\", attr, val, visual_range=[0, 300], visual_text_color=\"#fff\", is_geo_effect_show=False,\n",
    "                    is_piecewise=True, visual_split_number=6, symbol_size=15, is_visualmap=True)\n",
    "        except ValueError as e:\n",
    "            e = str(e)\n",
    "            e = e.split(\"No coordinate is specified for \")[1]  # 获取不支持的城市名称\n",
    "            info.pop(e)\n",
    "        else:\n",
    "            break\n",
    "    info = sorted(info.items(), key=lambda x: x[1], reverse=False)  # list排序\n",
    "    print(info)\n",
    "    info = dict(info)   # list转dict\n",
    "    print(info)\n",
    "    attr, val = [], []\n",
    "    for key in info:\n",
    "        attr.append(key)\n",
    "        val.append(info[key])\n",
    "\n",
    "\n",
    "    geo1 = Geo(\"\", \"评论城市分布\", title_pos=\"center\", width=1200, height=600,\n",
    "              background_color='#404a59', title_color=\"#fff\")\n",
    "    geo1.add(\"\", attr, val, visual_range=[0, 300], visual_text_color=\"#fff\", is_geo_effect_show=False,\n",
    "            is_piecewise=True, visual_split_number=10, symbol_size=15, is_visualmap=True, is_more_utils=True)\n",
    "    # geo1.render(csv_file + \"_城市dotmap.html\")\n",
    "    page.add_chart(geo1)\n",
    "    geo2 = Geo(\"\", \"评论来源热力图\",title_pos=\"center\", width=1200,height=600, background_color='#404a59', title_color=\"#fff\",)\n",
    "    geo2.add(\"\", attr, val, type=\"heatmap\", is_visualmap=True, visual_range=[0, 50],visual_text_color='#fff', is_more_utils=True)\n",
    "    # geo2.render(csv_file+\"_城市heatmap.html\")  # 取CSV文件名的前8位数\n",
    "    page.add_chart(geo2)\n",
    "    bar = Bar(\"\", \"评论来源排行\", title_pos=\"center\", width=1200, height=600 )\n",
    "    bar.add(\"\", attr, val, is_visualmap=True, visual_range=[0, 100], visual_text_color='#fff',mark_point=[\"average\"],mark_line=[\"average\"],\n",
    "            is_more_utils=True, is_label_show=True, is_datazoom_show=True, xaxis_rotate=45)\n",
    "    bar.render(csv_file+\"_城市评论bar.html\")  # 取CSV文件名的前8位数\n",
    "    page.add_chart(bar)\n",
    "    pie = Pie(\"\", \"评论来源饼图\", title_pos=\"right\", width=1200, height=600)\n",
    "    pie.add(\"\", attr, val, radius=[20, 50], label_text_color=None, is_label_show=True, legend_orient='vertical', is_more_utils=True, legend_pos='left')\n",
    "    pie.render(csv_file + \"_城市评论Pie.html\")  # 取CSV文件名的前8位数\n",
    "    page.add_chart(pie)\n",
    "    page.render(csv_file + \"_城市评论分析汇总.html\")\n",
    "    geo.render()\n",
    "    geo1.render()\n",
    "    geo2.render()\n",
    "    pie.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_draw(csv_file):\n",
    "    page = Page(csv_file+\":评论等级分析\")\n",
    "    score, date, val, score_list = [], [], [], []\n",
    "    result = {}\n",
    "    path = os.path.abspath(os.curdir)\n",
    "    csv_file = path + \"\\\\\" + csv_file + \".csv\"\n",
    "    csv_file = csv_file.replace('\\\\', '\\\\\\\\')\n",
    "    d = pd.read_csv(csv_file, engine='python', encoding='utf-8')[['score', 'date']].dropna()  # 读取CSV转为dataframe格式，并丢弃评论为空的记录\n",
    "    for indexs in d.index:  # 一种遍历df行的方法（下面还有第二种，iterrows）\n",
    "        score_list.append(tuple(d.loc[indexs].values[:])) # 目前只找到转换为tuple然后统计相同元素个数的方法\n",
    "    print(\"有效评分总数量为：\",len(score_list), \" 条\")\n",
    "    for i in set(list(score_list)):\n",
    "        result[i] = score_list.count(i)  # dict类型\n",
    "    info = []\n",
    "    for key in result:\n",
    "        score= key[0]\n",
    "        date = key[1]\n",
    "        val = result[key]\n",
    "        info.append([score, date, val])\n",
    "    info_new = DataFrame(info)  # 将字典转换成为数据框\n",
    "    info_new.columns = ['score', 'date', 'votes']\n",
    "    info_new.sort_values('date', inplace=True)    # 按日期升序排列df，便于找最早date和最晚data，方便后面插值\n",
    "    print(\"first df\", info_new)\n",
    "    # 以下代码用于插入空缺的数据，每个日期的评分类型应该有5中，依次遍历判断是否存在，若不存在则往新的df中插入新数值\n",
    "    mark = 0\n",
    "    creat_df = pd.DataFrame(columns = ['score', 'date', 'votes']) # 创建空的dataframe\n",
    "    for i in list(info_new['date']):\n",
    "        location = info_new[(info_new.date==i)&(info_new.score==\"力荐\")].index.tolist()\n",
    "        if location == []:\n",
    "            creat_df.loc[mark] = [\"力荐\", i, 0]\n",
    "            mark += 1\n",
    "        location = info_new[(info_new.date==i)&(info_new.score==\"推荐\")].index.tolist()\n",
    "        if location == []:\n",
    "            creat_df.loc[mark] = [\"推荐\", i, 0]\n",
    "            mark += 1\n",
    "        location = info_new[(info_new.date==i)&(info_new.score==\"还行\")].index.tolist()\n",
    "        if location == []:\n",
    "            creat_df.loc[mark] = [\"还行\", i, 0]\n",
    "            mark += 1\n",
    "        location = info_new[(info_new.date==i)&(info_new.score==\"较差\")].index.tolist()\n",
    "        if location == []:\n",
    "            creat_df.loc[mark] = [\"较差\", i, 0]\n",
    "            mark += 1\n",
    "        location = info_new[(info_new.date==i)&(info_new.score==\"很差\")].index.tolist()\n",
    "        if location == []:\n",
    "            creat_df.loc[mark] = [\"很差\", i, 0]\n",
    "            mark += 1\n",
    "    info_new = info_new.append(creat_df.drop_duplicates(), ignore_index=True)\n",
    "    score_list = []\n",
    "    info_new.sort_values('date', inplace=True)    # 按日期升序排列df，便于找最早date和最晚data，方便后面插值\n",
    "    print(info_new)\n",
    "    for index, row in info_new.iterrows():   # 第二种遍历df的方法\n",
    "        score_list.append([row['date'], row['votes'], row['score']])\n",
    "    tr = ThemeRiver()\n",
    "    tr.add(['力荐', '推荐', '还行', '较差', '很差'], score_list, is_label_show=True, is_more_utils=True)\n",
    "    page.add_chart(tr)\n",
    "\n",
    "    attr, v1, v2, v3, v4, v5 = [], [], [], [], [], []\n",
    "    attr = list(sorted(set(info_new['date'])))\n",
    "    bar = Bar()\n",
    "    for i in attr:\n",
    "        v1.append(int(info_new[(info_new['date']==i)&(info_new['score']==\"力荐\")]['votes']))\n",
    "        v2.append(int(info_new[(info_new['date']==i)&(info_new['score']==\"推荐\")]['votes']))\n",
    "        v3.append(int(info_new[(info_new['date']==i)&(info_new['score']==\"还行\")]['votes']))\n",
    "        v4.append(int(info_new[(info_new['date']==i)&(info_new['score']==\"较差\")]['votes']))\n",
    "        v5.append(int(info_new[(info_new['date']==i)&(info_new['score']==\"很差\")]['votes']))\n",
    "    bar.add(\"力荐\", attr, v1, is_stack=True)\n",
    "    bar.add(\"推荐\", attr, v2, is_stack=True)\n",
    "    bar.add(\"还行\", attr, v3, is_stack=True)\n",
    "    bar.add(\"较差\", attr, v4, is_stack=True)\n",
    "    bar.add(\"很差\", attr, v5, is_stack=True, is_convert=True, mark_line=[\"average\"], is_more_utils=True)\n",
    "    page.add_chart(bar)\n",
    "\n",
    "    line = Line()\n",
    "    line.add(\"力荐\", attr, v1, is_stack=True)\n",
    "    line.add(\"推荐\", attr, v2, is_stack=True)\n",
    "    line.add(\"还行\", attr, v3, is_stack=True)\n",
    "    line.add(\"较差\", attr, v4, is_stack=True)\n",
    "    line.add(\"很差\", attr, v5, is_stack=True, is_convert=False, mark_line=[\"average\"], is_more_utils=True)\n",
    "    page.add_chart(line)\n",
    "\n",
    "    page.render(csv_file[:-4] + \"_日投票量分析汇总.html\")\n",
    "    line.render_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(csv_file, stopwords_path, pic_path):\n",
    "    draw_sentiment_pic(csv_file)\n",
    "    draw_citys_pic(csv_file)\n",
    "    score_draw(csv_file)\n",
    "    word_cloud(csv_file,stopwords_path, pic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HiGirl\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyecharts\\base.py:173: UserWarning: Implementation has been removed. Please pass the chart instance directly to Jupyter.If you need more help, please read documentation\n",
      "  warnings.warn(\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\HiGirl\\AppData\\Local\\Temp\\jieba.cache\n",
      "DEBUG:jieba:Loading model from cache C:\\Users\\HiGirl\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.672 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.672 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "城市总数量为： 219\n",
      "[('南宁', 2), ('南昌', 2), ('无锡', 2), ('台州', 2), ('西安', 2), ('长沙', 2), ('重庆', 2), ('绍兴', 2), ('济南', 2), ('太原', 2), ('合肥', 2), ('珠海', 2), ('南京', 5), ('杭州', 6), ('成都', 7), ('广州', 8), ('深圳', 10), ('上海', 42), ('北京', 115)]\n",
      "{'南宁': 2, '南昌': 2, '无锡': 2, '台州': 2, '西安': 2, '长沙': 2, '重庆': 2, '绍兴': 2, '济南': 2, '太原': 2, '合肥': 2, '珠海': 2, '南京': 5, '杭州': 6, '成都': 7, '广州': 8, '深圳': 10, '上海': 42, '北京': 115}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Geo' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-76692cfdb661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"流浪地球\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"stopwords.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"胡歌.jpg\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-3386104fda0a>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(csv_file, stopwords_path, pic_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdraw_sentiment_pic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdraw_citys_pic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mscore_draw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mword_cloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstopwords_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-abd2a48cf863>\u001b[0m in \u001b[0;36mdraw_citys_pic\u001b[1;34m(csv_file)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_chart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_城市评论分析汇总.html\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mgeo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mgeo1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mgeo2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Geo' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(\"流浪地球\", \"stopwords.txt\", \"胡歌.jpg\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
